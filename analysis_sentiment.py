# -*- coding: utf-8 -*-
"""Analysis_Sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jzPeq77c1v8EjCy50IcJHRQy1l0zkzAq
"""

#import the libraries
import tweepy
from textblob import TextBlob
from wordcloud import WordCloud
import pandas as pd 
import numpy as np
import re
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

from keras.preprocessing.sequence import pad_sequences
import joblib
from sklearn.preprocessing import LabelEncoder
from keras.layers import Embedding
from keras.models import Sequential
from keras.layers import Dense,LSTM,Dropout
from sklearn.metrics import confusion_matrix, accuracy_score,classification_report

# Twitter API credentials
consumer_key= '57wfyVSkzCJP2WWOqeMNHGTrr'
consumer_secret= 'ANc4PRf4ggKmmfBAuo8iLv5qGRLacx0AaZZwLOria4ozMG9ZYw'
access_token= '1072640288147955712-gLBDm9lPnkUhrc3m4NEGyzu2NWaoHq'
access_token_secret= 'L7A98Mo8kfMwBVjnG8rZ8EcynpmlfRT6qRMgqZgjJslut'

#create the authentication object
authenticate = tweepy.OAuthHandler(consumer_key, consumer_secret)

#set the access token access secret
authenticate.set_access_token (access_token, access_token_secret)

#Create the API object while passing in teh auth information 
api = tweepy.API(authenticate, wait_on_rate_limit = True)

# hasil_search = api.search(q='vaksin', count=500, lang='en')
hasil_search = tweepy.Cursor(api.search, q='covid Jakarta -filter:retweets', tweet_mode='extended',lang="id").items(500)

result = []
for tweet in hasil_search :
  tweet_properties = {}
  tweet_properties['tanggal'] = tweet.created_at
  tweet_properties['user']=tweet.user.screen_name
  tweet_properties['cuitan'] = tweet.full_text
  tweet_bersih = ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\s+)"," ",tweet.full_text).split())
  analisis = TextBlob(tweet_bersih)
  try :
    analisis = analisis.translate(to="en")
  except Exception as e :
    print(e)

  tweet_properties['polarity'] = analisis.sentiment.polarity
  tweet_properties['subjectivity'] = analisis.sentiment.subjectivity

  if analisis.sentiment.polarity > 0.0 :
    tweet_properties['sentiment'] = 'Positif'
  elif analisis.sentiment.polarity == 0.0 :
    tweet_properties['sentiment'] = 'Netral'
  else :
    tweet_properties['sentiment'] = 'Negatif'


  if tweet.retweet_count > 0 :
    if tweet_properties not in result :
      result.append(tweet_properties)
  else :
    result.append(tweet_properties)

result

df = pd.DataFrame(result)

df

def cleaning_text(text) :
    text = re.sub(r'[^0-9A-Za-z \t]',r'' ,text) 
    text = re.sub(r'\b\w(1,2)\b',r'',text)
    text = re.sub(r'#','', text)
    text = re.sub(r'[@]',r' ',text)
    text = re.sub(r'\s\s+',r'',text)
    text = re.sub(r'\n',r'',text)
    return text

df['CleanTweet'] = df['cuitan'].apply(lambda x: cleaning_text(x))
df.head()

# tokenisasi 
def tokennization(text):
    text = re.split ('\W+', text)
    return text

df['Tokennization'] = df['CleanTweet'].apply(lambda x: tokennization(x.lower()))
df.head()

pip install nltk

# menghapus stopwords dan simpan pada stop_removal
import nltk
nltk.download('stopwords')
stopword = nltk.corpus.stopwords.words('indonesian')

def remove_stopwords(text):
  text = [word for word in text if word not in stopword]
  return text

df['Stop_removal'] = df['Tokennization'].apply(lambda x: remove_stopwords(x))

df.head()

tweet_positif = [t for t in result if t['sentiment'] == 'Positif']
tweet_netral = [t for t in result if t['sentiment'] == 'Netral']
tweet_negatif = [t for t in result if t['sentiment'] == 'Negatif']

print('Hasil Sentiment')
print("Positif : ", len(tweet_positif))
print("Netral : ", len(tweet_netral))
print("Negatif : ", len(tweet_negatif))

df['sentiment'].value_counts()
plt.title('Sentiment Analysis')
plt.xlabel('Sentiment')
plt.ylabel('Counts')
df['sentiment'].value_counts().plot(kind='bar')

#write your Scatter Plot Matplotlib here
fig,ax = plt.subplots()
ax.scatter(df['sentiment'], df['subjectivity'])
ax.set_title('Sentiment')
ax.set_xlabel('polarity')
ax.set_ylabel('subjectivity')

df['sentiment'].value_counts().sort_index().plot(kind = 'line',color = 'b',label = 'polarity',linewidth=1, alpha = 0.6,grid = True,linestyle = '-',figsize=(18,5))
plt.legend(loc='upper right')   
plt.xlabel('polarity',fontsize = 15,color='black')             
plt.ylabel('frekuensi',fontsize = 15,color='black')
plt.title('Sentiment',fontsize = 20,color='blue')            
plt.show()

df = df[['CleanTweet', 'polarity', 'subjectivity']]
df

Analysis = df['CleanTweet']
try:
  Analysis = str ( Analysis.get('Analysis', '' ) ) 
except Exception as e:
  print (e)

def getAnalysis(score) :
  if score < 0 :
    return -1
  elif score == 0 :
    return 0
  else : 
    return 1

df['Analysis'] = df['polarity'].apply(getAnalysis)

df.head()

# wordCloud
allWords = ' '.join( [twts for twts in df['CleanTweet']] )
wordCloud = WordCloud(width = 500, height = 300, random_state= 21, max_font_size=119).generate(allWords)

plt.imshow(wordCloud, interpolation = 'bilinear')
plt.axis('off')
plt.show()

nltk.download('all')

# ------ Case Folding --------
# gunakan fungsi Series.str.lower() pada Pandas
df['CleanTweet'] = df['CleanTweet'].str.lower()


df

import string 
import re #regex library

# import word_tokenize & FreqDist from NLTK
from nltk.tokenize import word_tokenize 
from nltk.probability import FreqDist

# ------ Tokenizing ---------

def remove_links(text):
    # remove tab, new line, ans back slice
    text = text.replace('\\t'," ").replace('\\n'," ").replace('\\u'," ").replace('\\',"")
    # remove non ASCII (emoticon, chinese word, .etc)
    text = text.encode('ascii', 'replace').decode('ascii')
    # remove mention, link, hashtag
    text = ' '.join(re.sub("([@#][A-Za-z0-9]+)|(\w+:\/\/\S+)"," ", text).split())
    # remove incomplete URL
    return text.replace("http://", " ").replace("https://", " ")
                
df['CleanTweet'] = df['CleanTweet'].apply(remove_links)

#remove number
def remove_number(text):
    return  re.sub(r"\d+", " ", text)

df['CleanTweet'] = df['CleanTweet'].apply(remove_number)

#remove punctuation
def remove_punctuation(text):
    return text.translate(str.maketrans("","",string.punctuation))

df['CleanTweet'] = df['CleanTweet'].apply(remove_punctuation)

# remove single char
def remove_singl_char(text):
    return re.sub(r"\b[a-zA-Z]\b", " ", text)

df['CleanTweet'] = df['CleanTweet'].apply(remove_singl_char)

# NLTK word tokenize 
def word_tokenize_wrapper(text):
    return word_tokenize(text)

df['Tokenize'] = df['CleanTweet'].apply(word_tokenize_wrapper)

df

# NLTK calc frequency distribution
def freqDist_wrapper(text):
    return FreqDist(text)

Ulasan_fqsist = df['Tokenize'].apply(freqDist_wrapper)

print('Frequency Tokens : \n') 
print(Ulasan_fqsist.head().apply(lambda x : x.most_common()))

from nltk.corpus import stopwords

list_stopwords = stopwords.words('indonesian')

#remove stopword pada list token
def stopwords_removal(words):
    return [word for word in words if word not in list_stopwords]

df['stop_removed'] = df['Tokenize'].apply(stopwords_removal)

df

pip install Sastrawi

pip install swifter

# import Sastrawi package
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import swifter


# create stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# stemmed
def stemmed_wrapper(term):
    return stemmer.stem(term)

term_dict = {}

for document in df['stop_removed']:
    for term in document:
        if term not in term_dict:
            term_dict[term] = ' '
            
print(len(term_dict))
print("------------------------")

for term in term_dict:
    term_dict[term] = stemmed_wrapper(term)
    print(term,":" ,term_dict[term])
    
print(term_dict)
print("------------------------")


# apply stemmed term to dataframe
def get_stemmed_term(document):
    return [term_dict[term] for term in document]

df['Ulasan_Stemmed'] = df['stop_removed'].swifter.apply(get_stemmed_term)

df

df["Clean"] = [' '.join(map(str, l)) for l in df['Ulasan_Stemmed']]

df

Tweet = ' '.join(str(v) for v in df['Clean'])

tokenize_tweet = word_tokenize(Tweet)

tokenize_tweet

fqdist = FreqDist(tokenize_tweet)
fqdist

df

df["Analysis"].value_counts()

data_label = df[["Clean", "Analysis"]]

sentimen_data=pd.value_counts(data_label["Analysis"], sort= True)
sentimen_data.plot(kind= 'bar', color= ["green", "red",'blue'])
plt.title('Bar chart')
plt.show()

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(decode_error='replace', encoding='utf-8')

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(data_label['Clean'], data_label['Analysis'], 
                                                    test_size=0.2, stratify=df['Analysis'], random_state=155)

X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

print(X_train.shape)
print(X_test.shape)

X_train = X_train.toarray()
X_test = X_test.toarray()

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold

#deklarasi metode cross validation
cv_method = RepeatedStratifiedKFold(n_splits=6,  n_repeats=3, random_state=999)
#tuning hyperparameter menggunakan gridsearch

params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}
gscv_nb = GridSearchCV(estimator=nb, 
                 param_grid=params_NB, 
                 cv=cv_method,   # use any cross validation technique 
                 verbose=1, 
                 scoring='accuracy') 

#Fitting ke Model
gscv_nb.fit(X_train,y_train)
#mendapatkan hyperparameters terbaik
gscv_nb.best_params_

nb = GaussianNB(var_smoothing=1.0)

nb.fit(X_train, y_train)

y_pred_nb = nb.predict(X_test)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

print('--------------------- classification report  ----------------------------')
print(classification_report(y_test, y_pred_nb))

print(accuracy_score)